---
title: "RQ2: artificialy attritioned datasets"
format: 
  html:
    fig-width: 8
    fig-height: 8
    code-link: true
    code-fold: true
    code-tools: true
    df-print: paged
    toc: true
    grid:
      body-width: 900px
editor: source
---

```{r}

source("0_load_data.R")

rq1y = rq1y[-1] # Remove year 2 time point from comparison list 

rq1y_labels = rq1y_labels[-1]

rq1y_labels_clean = rq1y_labels_clean[-1]

rq1y_labels_clean[8]

```

# Descriptive Statistics

```{r}
df %>%
  select(all_of(rq2y)) %>%
  # select(where(is.numeric)) %>%
  # mutate(across(everything(), as.character)) %>%
  pivot_longer(cols = everything()) %>%
  ggplot(aes(x=value)) +
  geom_histogram() +
  facet_wrap(~name, ncol = 4, scales = "free")


df %>%
  select(all_of(rq2y)) %>%
  apply(.,2,function(x) length(unique(x)))

```

# Create Attritioned Datasets

To remove participants, I've set the rows to NA rather than remove the rows, so all the datasets should have the same length.

```{r}

attritioned_datasets = list()

for (i in seq_along(rq1y)){
  
  filter = as.numeric(df[[rq1y[i]]])==0
  
  attritioned_datasets[[i]] = df %>% 
    select(all_of(rq2y))
  
  attritioned_datasets[[i]][filter,] = NA
  
}

original_dataset = df %>%
  select(all_of(rq2y))


```

# Compare univariate distributions

Load Results

I ran the bootstrapped analyses in the script [2_rq2_run_bootstrapping.R](2_rq2_run_bootstrapping.R){target="_blank"}.

```{r}

variable_comparisons    = readRDS(file = file.path("results", "2_variable_comparisons.Rds"))

variable_comparisons_df = readRDS(file = file.path("results", "2_variable_comparisons_df.Rds"))

# variable_comparisons_df = do.call("rbind.data.frame", variable_comparisons_df)

variable_comparisons_df = variable_comparisons_df %>% 
  mutate(
    dataset = rq1y_labels_clean[match(.$dataset, rq1y)],
    variable = rq2y_labels_short[match(.$variable, rq2y_labels)]
    ) %>%
  group_by(dataset, stat) %>%
  mutate(
    pval_adj = stats::p.adjust(pval, method = "holm")
  )

# Data checks 

# variable_comparisons_df %>% 
#   filter(stat == "smd" & dataset == "zmhdata") %>%
#   mutate(
#     pval_adj2 = stats::p.adjust(pval, method = "holm")
#   ) 

# variable_comparisons_df %>%
#   filter(stat == "smd") %>%
#   print(n=120)

# 
# x = variable_comparisons[[11]]$bootstrap_iter$smd[,1]
# 
# table(x>0) / sum(table(x))
# 
# bayestestR::pd(x) %>% bayestestR::pd_to_p(.)

```

Plot Hellingers and SMD

```{r}

variable_comparisons_df %>%
  mutate(sig = pval_adj < .05) %>%
  filter(stat == "smd") %>%
  mutate(dataset = factor(dataset, levels = unique(variable_comparisons_df$dataset))) %>%
  ggplot(aes(x = variable, y = y, ymin = ymin, ymax = ymax, fill = sig)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_col() +
  geom_errorbar() +
  facet_wrap(~dataset, scales = "fixed") +
  coord_flip() +
  theme_bw() +
  labs(
    title = "Attritioned - Original Variable Means (standardised mean differences)"
  )

ggsave(file.path("plots","2_difference_means.pdf"), width = 8, height = 6)

```


Plot data distributions

```{r}

attritioned_datasets_long = attritioned_datasets

for (i in seq_along(attritioned_datasets_long)){
  attritioned_datasets_long[[i]] = gather(as.data.frame(as.matrix(attritioned_datasets[[i]])))
  attritioned_datasets_long[[i]]$dataset = rq1y[i]
}

x = original_dataset %>%
    as.matrix() %>% as.data.frame() %>%
    gather() %>%
    mutate(dataset = "original")


attritioned_datasets_long = c(attritioned_datasets_long, list(x))

attritioned_datasets_long = do.call(rbind.data.frame,attritioned_datasets_long)

attritioned_datasets_long = attritioned_datasets_long %>%
  filter(!is.na(value))

attritioned_datasets_long %>%
  mutate(dataset = factor(dataset)) %>%
  ggplot(aes(x = value, group = dataset, col = dataset)) + 
  geom_density() + 
  facet_wrap(~key, scales = "free")

```

# Compare Correlation Structure between original and attrittioned data sets

First, some code to establish how to convert variable labels from matrix to dataframe formats.

```{r}
test_correlation_matrix = matrix(
  nrow = length(rq2y),
  ncol = length(rq2y)
  )

for(i in seq_along(rq2y)){
  for(j in seq_along(rq2y)){
    test_correlation_matrix[i,j] = paste(rq2y[i], rq2y[j], collapse = " ")
  }
}

x = test_correlation_matrix[lower.tri(test_correlation_matrix)]

x_var = str_extract(x, "^\\S+")
y_var = str_extract(x, "\\S+$")

rm(test_correlation_matrix,x)
```

Plot Code:

```{r}

correlation_comparisons = lapply(variable_comparisons, function(x) x$bootstrap_summary$cor_resid)

correlation_comparisons = lapply(correlation_comparisons, function(x) do.call(rbind.data.frame, x))

correlation_comparisons = lapply(correlation_comparisons, function(x) {
  x %>% 
    mutate(pval_adj = stats::p.adjust(pval, method = "holm"))
})

library(patchwork)

plots = lapply(1:11, function(i) {
  correlation_comparisons[[i]] %>%
    plot_lower_triangular_matrix(
      title = var_to_label(rq1y)[i] %>% 
        str_remove(", 1Y 0N") %>%
        stringr::str_wrap(., width = 35),
      
     caption = paste0("Variable name: ", rq1y[i])
    )
})

combined_plot = wrap_plots(plots, ncol = 4)  # Adjust ncol as needed

ggsave(
  filename = file.path("plots","2_rq2_2_correlations.pdf"), width = 16, height = 13,
  plot = combined_plot)
  
  

```

[ðŸ“Š View Full Resolution Correlation Plots (PDF)](plots/2_rq2_2_correlations.pdf)

# Compare ACE estimates between original and attritioned datasets

```{r}

ace_comparisons = readRDS(file = file.path("results", "2_ace_comparisons.Rds"))

# Extract ACE estimates from original (non-attrittioned) dataset

boot_results_original_df = lapply(ace_comparisons, function(x) x$boot_results_original_df)

boot_results_original_df = map_dfr(boot_results_original_df, ~ .x, .id = "variable")

boot_results_original_df_summary = boot_results_original_df %>%
  pivot_longer(cols = c(a,c,e)) %>%
  group_by(variable, name) %>%
  summarise(
    results = .mean_qi_pd(value),
    .groups = "drop"
  ) %>%
  unnest(cols = results)

# Extract changes in ACE parameters across bootstrap resamples

ace_comparisons_df = lapply(ace_comparisons, function(x) x$boot_results_differences_summary)

ace_comparisons_df = map_dfr(ace_comparisons_df, ~ map_dfr(.x, ~ map_dfr(.x, ~ .x, .id = "component"), .id = "dataset"), .id = "variable")

# Attritioned datasets: Extract mean estimates of ACE parameters across bootstrap resamples

ace_boot_results_attritioned_df = lapply(ace_comparisons, function(x) x$boot_results_attritioned_df)

ace_boot_results_attritioned_df = map_dfr(ace_boot_results_attritioned_df, ~ map_dfr(.x, ~ map_dfr(.x, ~ .x, .id = "component"), .id = "dataset"), .id = "variable")

ace_boot_results_attritioned_df %>% 
  pivot_longer(cols = c(a,c,e), names_to = "parameter") %>%
  group_by(variable, dataset, parameter) %>% 
  summarise(
    results = .mean_qi_pd(value),
    .groups = "drop"
  ) %>%
  unnest(results) 
  
```

Test for significant changes

```{r}

ace_comparisons_df

ace_comparisons_df %>% 
  ggplot(aes(x = pval)) + 
  geom_histogram(bins = 10) + 
  labs(
    title = "Histogram of p-values testing for changes in ACE parameters\nfrom original to attritioned groups"
  )

```

Ternery Plot

```{r}

rm(points_data, estimate_data, coordinates, var_colors)

pdf(file.path("plots","2_2_ace_ternary_plot.pdf"), width = 7, height = 7)

# Non-attritioned ACE estimates
original_ace_estimates = boot_results_original_df_summary %>%
  pivot_wider(values_from = y, names_from = name, id_cols = variable)

original_ace_estimates$variable_shortlabel = rq2y_labels_short[match(original_ace_estimates$variable, rq2y_prefix)]

attritioned_ace_estimates = ace_boot_results_attritioned_df %>%
  pivot_longer(cols = c(a,c,e), names_to = "parameter") %>%
  group_by(variable, dataset, parameter) %>%
  summarise(
    estimate = mean(value),
    .groups = "drop"
  ) %>%
  pivot_wider(names_from = parameter, values_from = estimate)

library(Ternary)

TernaryPlot(
  alab = "Additive Genetic (A)", 
  blab = "Common Environment (C)", 
  clab = "Unique Environment (E)",
  # main = title,
  grid.lines = 5,
  grid.col = "lightgray",
  axis.col = c("#D81B60", "#1E88E5", "#111111"),
  lab.col = c("#D81B60", "#1E88E5", "#111111")
)


var_colors = RColorBrewer::brewer.pal(length(rq2y), "Paired")

TernaryPoints(
  original_ace_estimates[c("a","c","e")], 
  pch = 16,  # filled circles
  cex = 1,
  col = var_colors[match(original_ace_estimates$variable, rq2y_prefix)]
)

TernaryPoints(
  attritioned_ace_estimates[,c("a","c","e")], 
  pch = 4,  
  cex = .9,
  col = var_colors[match(attritioned_ace_estimates$variable, rq2y_prefix)]
)

TernaryText(
  coordinates = original_ace_estimates[c("a","c","e")], #%>% mutate(c = c+.1,a=a+.1),
  labels = original_ace_estimates$variable_shortlabel,
  # col = var_colors[match(original_ace_estimates$variable, rq2y_prefix)],
  col = "black",
  cex = 0.5,           # Text size
  pos = 3,             # Position: 1=below, 2=left, 3=above, 4=right
  offset = 0.5         # Distance from point
)

dev.off()


```

[View ACE Ternary Plot](plots/2_2_ace_ternary_plot.pdf)


Why are the ACE estimates weird? 

```{r}

df %>%
  filter(random == 1) %>% 
  filter(x3zygos == "DZ same sex") %>%
  select(bsdqchypt1, bsdqchypt2) %>%
  {cor.test(.$bsdqchypt1, .$bsdqchypt2)}

df %>%
  filter(random == 1) %>% 
  filter(x3zygos == "MZ") %>%
  select(bsdqchypt1, bsdqchypt2) %>%
  {cor.test(.$bsdqchypt1, .$bsdqchypt2)}

df %>%
  filter(random == 1) %>% 
  filter(sexzyg == "DZ female") %>%
  select(bsdqchypt1, bsdqchypt2) %>%
  {cor.test(.$bsdqchypt1, .$bsdqchypt2)}

df %>%
  filter(random == 1) %>% 
  filter(sexzyg == "MZ female") %>%
  select(bsdqchypt1, bsdqchypt2) %>%
  {cor.test(.$bsdqchypt1, .$bsdqchypt2)}

df %>%
  filter(random == 1) %>% 
  filter(sexzyg == "DZ male") %>%
  select(bsdqchypt1, bsdqchypt2) %>%
  {cor.test(.$bsdqchypt1, .$bsdqchypt2)}

df %>%
  filter(random == 1) %>% 
  filter(sexzyg == "MZ male") %>%
  select(bsdqchypt1, bsdqchypt2) %>%
  {cor.test(.$bsdqchypt1, .$bsdqchypt2)}



```

# How predictive are our variables of later attrittion?

```{r}



models = list()

for (i in seq_along(rq1y)){
  cat(i, "/", length(rq1y),"\n")
  formula <- as.formula(paste(rq1y[i], "~", paste(rq2y, collapse = "+")))
  models[[i]] = glm(formula, data = df, family = binomial, na.action = "na.omit")
}


# glm_model_comparison(models[[1]])
# 
# lapply(models, function(x) performance::r2(x))
# lapply(models, function(x) calc_auc(x))
# lapply(models, function(x) summary(x))



# calc_auc(models[[1]])

```


### Plot of AUC and R2 for each timepoint 

```{r}

# Extract AUC and pseudo R2 for each model
model_metrics = data.frame(
  outcome = sapply(models, function(x) as.character(x$formula)[2]),
  AUC = sapply(models, calc_auc),
  R2 = sapply(models, function(x) performance::r2(x)$R2)
) %>%
  mutate(outcome = clean_rq1y_label(var_to_label(outcome)))

# First, order the outcomes chronologically by extracting year numbers
model_metrics_ordered = model_metrics %>%
  mutate(
    year_num = as.numeric(str_extract(outcome, "\\d+")),
    outcome = fct_reorder(outcome, year_num)
  )

plot_data = model_metrics_ordered %>%
  pivot_longer(cols = c(AUC, R2)) %>%
  mutate(
    # Format labels
    label_text = ifelse(name == "R2", 
                       paste0(sprintf("%.1f", value * 100), "%"),
                       sprintf("%.3f", value)),
    label_text = ifelse(name == "AUC", 
                       gbtoolbox::apa_num(value, n_decimal_places = 3),
                       label_text),
    # Adjust bar start position for AUC (start from 0.5 instead of 0)
    value_start = ifelse(name == "AUC", 0.5, 0),
    value_width = value - value_start
  )

plot_data %>%
  ggplot(aes(y = outcome)) +
  geom_rect(aes(xmin = value_start, xmax = value, ymin = as.numeric(outcome) - 0.4, ymax = as.numeric(outcome) + 0.4), 
            fill = "steelblue", alpha = 0.7) + 
  geom_text(aes(x = value, label = label_text), hjust = -0.1, size = 3) +
  facet_wrap(~name, scales = "free", 
             labeller = labeller(name = c("AUC" = "AUC", "R2" = "Tjur's RÂ²"))) +
  scale_x_continuous(
    labels = function(x) {
      # Check if we're in the R2 facet by looking at the range
      if (max(x, na.rm = TRUE) < 0.2) {  # R2 values are typically small
        paste0(round(x * 100, 1), "%")
      } else {
        sprintf("%.2f", x)
      }
    },
    expand = expansion(mult = c(0, 0.15)),
    limits = function(x) {
      # Set minimum limit to 0.5 for AUC facet (values > 0.2)
      if (max(x, na.rm = TRUE) > 0.2) {
        c(0.5, max(x, na.rm = TRUE) * 1.05)
      } else {
        c(0, max(x, na.rm = TRUE) * 1.05)
      }
    }
  ) +
  labs(
    title = "Prediction Accuracy Across Study Timepoints",
    x = "Value",
    y = "Study Timepoint"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(face = "bold"),
    plot.title = element_text(hjust = 0.5)
  )

ggsave(file.path("plots", "2_prediction_accuracy.pdf"), width = 9, height = 6)

```

## Full model results 

```{r}

# Extract all coefficients and combine
models_results = do.call(rbind, lapply(1:length(models), function(i) {
  coefs = tidy(models[[i]])
  coefs$outcome = as.character(models[[i]]$formula)[2]
  return(coefs)
}))

models_results %>% 
  select(outcome, everything()) %>%
  mutate(
    outcome = var_to_label(outcome) %>% str_remove("data.*"),
    term = df_labels[match(.$term,df_colnames)],
    term   = ifelse(is.na(term), "Intercept", term),
    p_star = as.character(symnum(p.value, cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c("***", "**", "*", ".", " ")))
  ) %>% 
  knitr::kable(., digits = 3)


```


## Variable Importance Calculations

```{r}
glm_model_comparison_results = lapply(models, glm_model_comparison)

glm_model_comparison_results_df = do.call("rbind.data.frame", glm_model_comparison_results)

# glm_model_comparison_results_df$LRT_p_star = glm_model_comparison_results_df$LRT_p %>%
#   symnum(., cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c("***", "**", "*", ".", " "))

# Adjust p-values and add star

glm_model_comparison_results_df = glm_model_comparison_results_df %>% 
  filter(Variables_Dropped!="None") %>%
  group_by(outcome) %>%
  mutate(LRT_p = stats::p.adjust(LRT_p, method = "holm")) %>% 
  ungroup() %>%
  mutate(
    LRT_p_star = as.character(symnum(LRT_p, cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), symbols = c("***", "**", "*", ".", " ")))
  )

variable_importance_order = glm_model_comparison_results_df %>%
  group_by(Variables_full) %>%
  summarise(mean_score = mean(LRT_p)) %>%
  arrange(mean_score) %>%
  pull(Variables_full)
glm_model_comparison_results_df = glm_model_comparison_results_df %>%
  mutate(
    Variables_full = factor(Variables_full, levels = variable_importance_order)
  )

# Plotting code
glm_model_comparison_results_df %>%
  mutate(
    outcome_labeled = var_to_label(outcome),
    outcome_labeled = outcome_labeled %>% str_remove("data.*"),
    outcome_labeled = factor(outcome_labeled, levels =  str_remove(var_to_label(rq1y),"data.*")),
    criterion       = Delta_AUC,
    sig_level = case_when(
      LRT_p < 0.001 ~ "p < 0.001",
      LRT_p < 0.01 ~ "p < 0.01", 
      LRT_p < 0.05 ~ "p < 0.05",
      LRT_p < 0.1 ~ "p < 0.1",
      TRUE ~ "Not significant"
    ),
    sig_level = factor(sig_level, levels = c("p < 0.001", "p < 0.01", "p < 0.05", "p < 0.1", "Not significant"))
  ) %>%
  ggplot(aes(y = Variables_full, x = criterion)) + 
  geom_col(aes(fill = sig_level), alpha = 0.8) + 
  geom_text(aes(label = gsub("0\\.", ".", sprintf("%.3f", criterion))),  # Remove ALL 0. patterns
            size = 2.5, fontface = "bold", color = "black") +
  facet_wrap(~outcome_labeled, scales = "fixed", ncol = 6) +
  scale_x_continuous(labels = function(x) gsub("0\\.", ".", sprintf("%.3f", x))) +  # Remove ALL 0. patterns
  scale_fill_manual(
    values = c("p < 0.001" = "#d73027", "p < 0.01" = "#fc8d59", 
               "p < 0.05" = "#fee08b", "p < 0.1" = "#e0f3f8", 
               "Not significant" = "#d9d9d9"),
    name = "Significance\n(Holm-adjusted)"
  ) +
  labs(
    x = "Change in AUC (when variable removed)",
    y = NULL,
    title = "Variable Importance by AUC Change",
    subtitle = "Colors show Holm-adjusted significance levels"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 7),
    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
    strip.text = element_text(size = 9, face = "bold"),
    legend.position = "bottom"
  )


ggsave(file.path("plots", "2_rq2_variable_importance.pdf"), width = 14, height = 7)
```



# Random / Code Checks

```{r}
#| eval: false 

# Hellingers distances

# Small

statip::hellinger(rnorm(100000,1,1),rnorm(100000,1.2,1))*sqrt(2)

# Medium

statip::hellinger(rnorm(100000,1,1),rnorm(100000,1.5,1))*sqrt(2)

# Large

statip::hellinger(rnorm(100000,1,1),rnorm(100000,2,1))*sqrt(2)

# Huge

statip::hellinger(rnorm(100000,1,1),rnorm(100000,1,10))*sqrt(2)

# More tests

distrEx::HellingerDist(rnorm(1000,1,1),rnorm(1000,3,1), asis.smooth.discretize = "smooth")


HellingerDist(c(.3,.5,.2),c(.3,.6,.3))

statip::hellinger(rnorm(100000,1,1),rnorm(100000,3,1))*sqrt(2)

HellingerDist(rbinom(50, size = 20, prob = 0.5), Binom(size = 20, prob = 0.5))

apply(attritioned_datasets[[1]],2, function(x) length(which(!is.na(x))))


# rq2y
# rq1y[10]
# 
# calc_smd(
# cleanvar(original_dataset$bvocab1),
# cleanvar(attritioned_datasets[[9]]$bvocab1)
# )
# 
# calc_md(
# cleanvar(original_dataset$bvocab1),
# cleanvar(attritioned_datasets[[9]]$bvocab1)
# )
# 
# calc_smd(
# cleanvar(original_dataset$bvocab1),
# cleanvar(attritioned_datasets[[9]]$bvocab1)
# )
# 
# calc_smd(
# cleanvar(original_dataset$bvocab1),
# cleanvar(attritioned_datasets[[4]]$bvocab1)
# )
# 
# x=compare_df(
#   attritioned_datasets[[9]],
#   original_dataset
# )
# 
# x=compare_smd(
#   attritioned_datasets[[9]],
#   original_dataset
# )

# compare_smd(
#   attritioned_datasets[[9]],
#   original_dataset
# )

calc_smd(
   cleanvar(attritioned_datasets[[9]]$brawg1),
   cleanvar(original_dataset$brawg1)
)

statip::hellinger(
   cleanvar(attritioned_datasets[[9]]$brawg1),
   cleanvar(original_dataset$brawg1)
)



variable_comparisons_df %>%
  filter(dataset == "rcqdata") %>%
  filter(stat != "md") 

rq1y[9]
```
